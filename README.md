Hi! This is an application that I created to work with the Ollama model and keep a history of interactions. It can answer your questions using the locally running Ollama API, and also allows you to search through saved questions using the ChromaDB database. What does this app do? First, you ask a question in a text field. This question is passed to the Ollama API, which returns the answer to you. All your questions are saved in the database so that they can be found later. At any time, you can use the search function to remember what you asked earlier. ChromaDB is used to store the history. I made it so that each request is converted into a vector representation (embedding), and then saved along with the text. This allows you to quickly find queries similar to your search query. How does it work? I used Streamlit as the interface. This allows you to launch the application directly in the browser, and everything looks clear and simple.
The Ollama API is responsible for processing your requests. You enter a text, it is passed to the model, and the model returns a response. This works locally, so it's important that the Ollama API is running on your computer.
I used ChromaDB to store the data. Each question is saved in the database as text and its vector representation. So far, I'm using random numbers to create embedding vectors, but in the future, I can connect a real model for better search.
When you search for something from the history, the application converts your query into a vector and compares it with the vectors of previously saved questions. As a result, you see the queries that are closest to your current search.
How to use it?
When you launch the application, you will have two text fields. In the first field, you enter your question. After pressing Enter, the question is sent to the Ollama API, and you receive an answer. The same question is automatically saved to the database so that you can find it later. In the second field, you can search by history. For example, if you have ever asked a question about the weather, you can enter the word "weather" and the app will show all the related questions you have asked.
Right now, the application uses random numbers to create embedding vectors that are used for searching. This means that the search results may not always be accurate. In the future, you can connect a real model to create more meaningful vectors, which will make the search much better. It is also important that the Ollama API is running on your computer at http://localhost:11434 . Without this, the app will not be able to answer questions. It is a simple but functional application. It is suitable as a basis for more complex projects. If you want, you can improve it: connect a real model to create an embedding, make the interface more beautiful, or add new features. I hope it will be useful to you!
